{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1c42c0-56a2-499c-abaa-881691adefb6",
   "metadata": {},
   "source": [
    "# ML Modeling — Predicting Patient Treatment Outcomes\n",
    "\n",
    "**Goal:**  \n",
    "Predict `outcome` (`Improved`, `Stable`, `Worsened`) using demographic + clinical features from your warehouse tables.\n",
    "\n",
    "- Focus modeling on **most predictive features** (`medical_condition`, `treatment`, `length_of_stay`, `age_band`)\n",
    "- Simplify weak/noisy features  \n",
    "  - Collapse `gender` into 3 groups (`Male`, `Female`, `Other`)  \n",
    "  - Group rare `medical_condition` and `treatment` categories into `Other`\n",
    "- Encode categorical features using :contentReference[oaicite:0]{index=0} `ColumnTransformer`\n",
    "- Evaluate 3 models: `Logistic Regression`, `Random Forest`, `XGBoost`\n",
    "- Include feature importance analysis to understand which features drive predictions\n",
    "\n",
    "## Plan \n",
    "\n",
    "**Step 1 — Connect & Load Data**\n",
    "- Pull joined `fact_encounter` + dimension tables from :contentReference[oaicite:4]{index=4} into a single dataframe\n",
    "\n",
    "**Step 2 — Feature Engineering & Cleanup**\n",
    "- Simplify and group noisy categories\n",
    "- Engineer new feature: `los_band` from `length_of_stay`\n",
    "- Handle missing values\n",
    "\n",
    "**Step 3 — Train/Test Split**\n",
    "- 80/20 split with stratification on `outcome`\n",
    "\n",
    "**Step 4 — Encode Features**\n",
    "- Use one-hot encoding for all categorical predictors\n",
    "\n",
    "**Step 5 — Train Models**\n",
    "- Fit Logistic Regression, Random Forest, and XGBoost classifiers\n",
    "- Evaluate accuracy, precision, recall, f1-score\n",
    "\n",
    "**Step 6 — Feature Importance**\n",
    "- Plot top predictors from Random Forest to interpret model behavior\n",
    "\n",
    "**(Next: Step 7 — Hyperparameter tuning)**  \n",
    "- Tune model hyperparameters (especially Random Forest and XGBoost) for better performance\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "- Baseline accuracy around **0.35–0.45** is acceptable given balanced 3-class setup\n",
    "- Insights about which patient features most influence treatment outcome\n",
    "- Ready-to-use encoded dataset for further model experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86d236be-cdc0-46d0-9345-dcb19d300cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect data\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# local PostgreSQL\n",
    "engine = create_engine(\"postgresql+psycopg2://dquser:dqpass@localhost:5432/healthcare\")\n",
    "\n",
    "# fact+dim join \n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    f.encounter_key,\n",
    "    p.patient_id, p.gender, p.insurance_type, p.smoking_status, p.income_band,\n",
    "    h.hospital_id, h.region,\n",
    "    ct.medical_condition, ct.treatment,\n",
    "    a.age_band,\n",
    "    f.length_of_stay,\n",
    "    f.outcome\n",
    "FROM warehouse.fact_encounter f\n",
    "JOIN warehouse.dim_patient p ON f.patient_key = p.patient_key\n",
    "JOIN warehouse.dim_hospital h ON f.hospital_key = h.hospital_key\n",
    "JOIN warehouse.dim_condition_treatment ct ON f.ct_key = ct.ct_key\n",
    "JOIN warehouse.dim_ageband a ON f.ageband_key = a.ageband_key\n",
    "\"\"\"\n",
    "df = pd.read_sql(text(query), engine)\n",
    "df.head()\n",
    "\n",
    "ml_df = df_eda.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "441c9bad-81cb-4100-bdfc-036a5eda7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost OK: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(\"XGBoost OK:\", xgb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec2778e9-5b7e-4ba5-b9a9-5a9b085300fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls per feature after fill:\n",
      "gender_simplified        0\n",
      "insurance_type           0\n",
      "smoking_status           0\n",
      "income_band              0\n",
      "region                   0\n",
      "medical_condition_grp    0\n",
      "treatment_grp            0\n",
      "age_band                 0\n",
      "los_band                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "\n",
    "# Simplify gender\n",
    "male_like = {\"Male\"}\n",
    "female_like = {\"Female\"}\n",
    "ml_df[\"gender_simplified\"] = ml_df[\"gender\"].where(\n",
    "    ml_df[\"gender\"].isin(male_like | female_like), other=\"Other\"\n",
    ")\n",
    "\n",
    "# Group rare medical_conditions (<20 rows)\n",
    "cond_counts = ml_df[\"medical_condition\"].value_counts()\n",
    "rare_conditions = cond_counts[cond_counts < 20].index\n",
    "ml_df[\"medical_condition_grp\"] = ml_df[\"medical_condition\"].replace(\n",
    "    dict.fromkeys(rare_conditions, \"Other\")\n",
    ")\n",
    "\n",
    "# Group rare treatments (<20 rows)\n",
    "treat_counts = ml_df[\"treatment\"].value_counts()\n",
    "rare_treatments = treat_counts[treat_counts < 20].index\n",
    "ml_df[\"treatment_grp\"] = ml_df[\"treatment\"].replace(\n",
    "    dict.fromkeys(rare_treatments, \"Other\")\n",
    ")\n",
    "\n",
    "# Bin length_of_stay into categories\n",
    "ml_df[\"los_band\"] = pd.cut(\n",
    "    ml_df[\"length_of_stay\"],\n",
    "    bins=[-1, 3, 7, 14, 100],\n",
    "    labels=[\"Short\", \"Medium\", \"Long\", \"Very Long\"]\n",
    ")\n",
    "\n",
    "# Strong features only\n",
    "target = \"outcome\"\n",
    "features = [\n",
    "    \"gender_simplified\", \"insurance_type\", \"smoking_status\",\n",
    "    \"income_band\", \"region\",\n",
    "    \"medical_condition_grp\", \"treatment_grp\", \"age_band\",\n",
    "    \"los_band\"\n",
    "]\n",
    "\n",
    "# NAs\n",
    "for col in ml_df.columns:\n",
    "    if isinstance(ml_df[col].dtype, pd.CategoricalDtype):\n",
    "        X = ml_df[features]\n",
    "        y = ml_df[target]\n",
    "\n",
    "print(\"Nulls per feature after fill:\")\n",
    "print(X.isna().sum())\n",
    "\n",
    "X = ml_df[features]\n",
    "y = ml_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449edeea-0159-4cb0-a41d-a468e4c28eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (800, 8) | Test size: (200, 8)\n",
      "\n",
      "Class balance (train):\n",
      "outcome\n",
      "Improved    36.2\n",
      "Worsened    32.4\n",
      "Stable      31.4\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class balance (test):\n",
      "outcome\n",
      "Improved    36.5\n",
      "Worsened    32.5\n",
      "Stable      31.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \"| Test size:\", X_test.shape)\n",
    "print(\"\\nClass balance (train):\")\n",
    "print(y_train.value_counts(normalize=True).mul(100).round(1))\n",
    "print(\"\\nClass balance (test):\")\n",
    "print(y_test.value_counts(normalize=True).mul(100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef57c73-229e-4ac1-9438-790e672d96e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train shape: (800, 47) | Encoded test shape: (200, 47)\n"
     ]
    }
   ],
   "source": [
    "# Build Preprocessing\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"cat\", ohe, features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "Xtr = preprocessor.transform(X_train)\n",
    "Xte = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Encoded train shape:\", Xtr.shape, \"| Encoded test shape:\", Xte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66d10e-7408-45fd-9db3-e225dc33092b",
   "metadata": {},
   "source": [
    "**Preprocessing Summary:** \n",
    "- Dataset: 1000 patient encounters (cleaned and analytics-ready)\n",
    "- Target: `outcome` (3-class classification)\n",
    "- Features used:  \n",
    "  - `gender`, `insurance_type`, `smoking_status`, `income_band`, `region`\n",
    "  - `medical_condition`, `treatment`, `age_band`, `hospital_id`\n",
    "- Train/Test Split: 80% / 20%  \n",
    "  - Train: 800 rows\n",
    "  - Test: 200 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "289d3bdd-e81e-4703-b5a0-59c49df64f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Accuracy: 0.270\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Improved       0.31      0.27      0.29        73\n",
      "      Stable       0.26      0.32      0.29        62\n",
      "    Worsened       0.24      0.22      0.23        65\n",
      "\n",
      "    accuracy                           0.27       200\n",
      "   macro avg       0.27      0.27      0.27       200\n",
      "weighted avg       0.27      0.27      0.27       200\n",
      "\n",
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.270\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Improved       0.31      0.30      0.31        73\n",
      "      Stable       0.24      0.27      0.26        62\n",
      "    Worsened       0.26      0.23      0.24        65\n",
      "\n",
      "    accuracy                           0.27       200\n",
      "   macro avg       0.27      0.27      0.27       200\n",
      "weighted avg       0.27      0.27      0.27       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define and Train models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Simple, regularized multinomial logistic regression\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=0.5,                 # a bit more regularization\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "print(\"\\n--- Logistic Regression ---\")\n",
    "logreg.fit(Xtr, y_train)\n",
    "preds = logreg.predict(Xte)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, preds))\n",
    "results[\"Logistic Regression\"] = acc\n",
    "\n",
    "# Smaller, shallower random forest to avoid overfitting sparse OHE features\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "rf.fit(Xtr, y_train)\n",
    "preds = rf.predict(Xte)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, preds))\n",
    "results[\"Random Forest\"] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d54d36-cba1-490f-a646-731285a5ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost ---\n",
      "Accuracy: 0.320\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Improved       0.35      0.41      0.38        73\n",
      "      Stable       0.29      0.31      0.30        62\n",
      "    Worsened       0.30      0.23      0.26        65\n",
      "\n",
      "    accuracy                           0.32       200\n",
      "   macro avg       0.32      0.32      0.31       200\n",
      "weighted avg       0.32      0.32      0.32       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Encode y for XGBoost ONLY\n",
    "le = LabelEncoder()\n",
    "y_train_num = le.fit_transform(y_train)  # ['Improved','Stable','Worsened'] -> [0,1,2]\n",
    "y_test_num  = le.transform(y_test)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(le.classes_)  # 3\n",
    ")\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "xgb_clf.fit(Xtr, y_train_num)\n",
    "preds_num = xgb_clf.predict(Xte)\n",
    "preds = le.inverse_transform(preds_num)  # map back to original labels\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, preds))\n",
    "results[\"XGBoost\"] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007b3a9e-cd6b-414b-ac12-cf8fe2e2171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary (accuracy) ---\n",
      "Logistic Regression: 0.300\n",
      "Random Forest: 0.325\n",
      "XGBoost: 0.320\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Summary (accuracy) ---\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (healthcare-hub)",
   "language": "python",
   "name": "healthcare-hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
